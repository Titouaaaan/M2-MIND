{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5a6d9d",
   "metadata": {},
   "source": [
    " Copyright Â© Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4451fbce",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, using BBRL, we code a version of the DQN algorithm with a\n",
    "replay buffer and a target network, using the AutoReset approach.\n",
    "\n",
    "To understand this code, you need to know more about\n",
    "[the BBRL interaction model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md)\n",
    "Then you should run [a didactical example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/03-multi_env_autoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=True.\n",
    "\n",
    "The DQN algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=CXwvOMJujZk) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/dqn.pdf).\n",
    "\n",
    "In this notebook, we will modify the previous notebook:\n",
    "\n",
    "- to use a replay buffer and an environment that resets\n",
    "- to use a target network for $Q$\n",
    "- to use a better estimation for the maximum (Double-DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c711c5d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# # Prepare the environment\n",
    "\n",
    "\n",
    "import bbrl_utils\n",
    "\n",
    "bbrl_utils.setup()\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, copy_parameters\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ebe1a6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Learning environment\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The learning environment is controlled by a configuration providing a few\n",
    "important things as described in the example below. This configuration can\n",
    "hold as many extra information as you need, the example below is the minimal\n",
    "one.\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # This defines the a path for logs and saved models\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # The Gymnasium environment\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # Algorithm\n",
    "    \"algorithm\": {\n",
    "        # Seed used for the random number generator\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # Number of parallel training environments\n",
    "        \"n_envs\": 8,\n",
    "\n",
    "        # Number of transitions to collect at each epoch for an environment.\n",
    "        # This number has to be multiplied by n_envs to get the number of new transitions\n",
    "        # collected at each epoch.\n",
    "        \"n_steps\": 100,\n",
    "\n",
    "        # Number of transitions before starting to train\n",
    "        \"learning_starts\": 10_000,\n",
    "                \n",
    "        # Minimum number of steps between two evaluations\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # Number of parallel evaluation environments\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # Number of epochs (loops)\n",
    "        \"max_epochs\": 40000,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Creates the configuration object, i.e. cfg.algorithm.nb_evals is 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "## The RL algorithm\n",
    "\n",
    "In this notebook, the RL algorithm is based on `EpochBasedAlgo`, that defines\n",
    "the algorithm environment when using episodes. To use such environment, we\n",
    "just need to subclass `EpochBasedAlgo` and to define two things, namely the\n",
    "`train_policy` and the `eval_policy`. Both are BBRL agents that, given the\n",
    "environment state, select the action to perform.\n",
    "\n",
    "```py\n",
    "  class MyAlgo(EpochBasedAlgo):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__(cfg)\n",
    "\n",
    "          # Define the train and evaluation policies\n",
    "          # (the agents compute the workspace `action` variable)\n",
    "          self.train_policy = MyPolicyAgent(...)\n",
    "          self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "The `EpochBasedAlgo` defines useful objects:\n",
    "\n",
    "- `algo.cfg` is the configuration\n",
    "- `algo.nb_steps` (integer) is the number of steps since the training began\n",
    "- `algo.logger` is a logger that can be used to collect statistics during training:\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` registers the `critic_loss` value on tensorboard\n",
    "- `algo.evaluate()` evaluates the current `eval_policy` if needed, and keeps the\n",
    "agent if it was the best so far (average cumulated reward);\n",
    "- `algo.visualize_best()` runs the best agent on one episode, and displays the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11114b8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Definition of agents\n",
    "\n",
    "In the following, we define three agents:\n",
    "1. `DiscreteQAgent` the computes the Q-values\n",
    "2. `ArgmaxActionSelector` that selects the action that maximizes the Q-value\n",
    "3. `EGreedyActionSelector` which behaves randomly with probability $\\epsilon$,\n",
    "   and otherwise behaves as `ArgmaxActionSelector`\n",
    "\n",
    "Note that contrarily to previous notebooks, we work here with $n$ environments\n",
    "in parallel (first dimension of each tensor)\n",
    "\n",
    "### The critic agent\n",
    "\n",
    "The [DQN](https://daiwk.github.io/assets/dqn.pdf) algorithm is a critic only\n",
    "algorithm. Thus we just need a Critic agent (which will also be used to output\n",
    "actions) and an Environment agent. We reuse the `DiscreteQAgent` class that we\n",
    "have already explained in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ee72b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class DiscreteQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = build_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        q_values = self.model(obs)\n",
    "        self.set((f\"{self.prefix}q_values\", t), q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f8109",
   "metadata": {},
   "source": [
    "### Creating an actor agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5dc8fd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ArgmaxActionSelector(Agent):\n",
    "    \"\"\"BBRL agent that selects the best action based on Q(s,a)\"\"\"\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        q_values = self.get((\"q_values\", t))\n",
    "        action = q_values.argmax(-1)\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ccf424",
   "metadata": {},
   "source": [
    "### Creating an Exploration method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55832738",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "As Q-learning, DQN needs some exploration to prevent too early convergence.\n",
    "Here we will use the simple $\\epsilon$-greedy exploration method. The method\n",
    "is implemented as an agent which chooses an action based on the Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e861cc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class EGreedyActionSelector(Agent):\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        # Retrieves the q values\n",
    "        # (matrix nb. of episodes x nb. of actions)\n",
    "        q_values: torch.Tensor = self.get((\"q_values\", t))\n",
    "        size, nb_actions = q_values.shape\n",
    "\n",
    "        # Flag\n",
    "        is_random = torch.rand(size) < self.epsilon\n",
    "\n",
    "        # Actions (random / argmax)\n",
    "        random_action = torch.randint(nb_actions, size=(size,))\n",
    "        max_action = q_values.argmax(-1)\n",
    "\n",
    "        # Choose the action based on the is_random flag\n",
    "        action = torch.where(is_random, random_action, max_action)\n",
    "\n",
    "        # Sets the action at time t\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca0eb82",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Main training loop\n",
    "\n",
    "Note that everything about the shared workspace between all the agents is\n",
    "completely hidden under the hood. This results in a gain of productivity, at\n",
    "the expense of having to dig into the BBRL code if you want to understand the\n",
    "details, change the multiprocessing model, etc.\n",
    "\n",
    "Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`\n",
    "lines. `optimizer.zero_grad()` is necessary to cancel all the gradients\n",
    "computed at the previous iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b99e1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class DQN(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "\n",
    "        # Get the two agents (critic and target critic)\n",
    "        critic = DiscreteQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.hidden_size, act_size\n",
    "        )\n",
    "        target_critic = copy.deepcopy(critic).with_prefix(\"target/\")\n",
    "\n",
    "        # Builds the train agent that will produce transitions\n",
    "        explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
    "        self.train_policy = Agents(critic, explorer)\n",
    "\n",
    "        self.eval_policy = Agents(critic, ArgmaxActionSelector())\n",
    "\n",
    "        # Creates two temporal agents just for \"replaying\" some parts\n",
    "        # of the transition buffer\n",
    "        self.t_q_agent = TemporalAgent(critic)\n",
    "        self.t_target_q_agent = TemporalAgent(target_critic)\n",
    "\n",
    "        # Get an agent that is executed on a complete workspace\n",
    "        self.optimizer = setup_optimizer(cfg.optimizer, self.t_q_agent)\n",
    "\n",
    "        self.last_critic_update_step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c1993",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Compute critic loss\n",
    "\n",
    "Detailed explanations of the function to compute the critic loss when using\n",
    "`autoreset=False` are given in [this\n",
    "notebook](http://master-dac.isir.upmc.fr/rld/rl/03-1-dqn-introduction.student.ipynb).\n",
    "The case where we use `autoreset=True` is very similar, but we need to specify\n",
    "that we use the first part of the Q-values (`q_values[0]`) for representing\n",
    "$Q(s_t,a_t)$ and the second part (`q_values[1]`) for representing\n",
    "$Q(s_{t+1},a)$, as these values are stored into a transition buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4585bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def dqn_compute_critic_loss(\n",
    "    cfg, reward, must_bootstrap, q_values, target_q_values, action\n",
    "):\n",
    "    \"\"\"Compute the critic loss\n",
    "\n",
    "    :param reward: The reward $r_t$ (shape 2 x B)\n",
    "    :param must_bootstrap: The must bootstrap flag at $t+1$ (shape 2 x B)\n",
    "    :param q_values: The Q-values (shape 2 x B x A)\n",
    "    :param target_q_values: The target Q-values (shape 2 x B x A)\n",
    "    :param action: The chosen actions (shape 2 x B)\n",
    "    :return: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement the DQN loss\n",
    "\n",
    "    # Adapt from the previous notebook and adapt to our case (target Q network)\n",
    "    # Don't forget that we deal with transitions (and not episodes)\n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "    # Compute critic loss (no need to use must_bootstrap here since we are dealing with \"full\" transitions)\n",
    "    mse = nn.MSELoss()\n",
    "    critic_loss = mse(target, qvals)\n",
    "\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35697e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733521e1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def run(dqn: DQN, compute_critic_loss):\n",
    "    for rb in dqn.iter_replay_buffers():\n",
    "        for _ in range(dqn.cfg.algorithm.n_updates):\n",
    "            rb_workspace = rb.get_shuffled(dqn.cfg.algorithm.batch_size)\n",
    "\n",
    "            # The q agent needs to be executed on the rb_workspace workspace\n",
    "            dqn.t_q_agent(rb_workspace, t=0, n_steps=2)\n",
    "            with torch.no_grad():\n",
    "                dqn.t_target_q_agent(rb_workspace, t=0, n_steps=2)\n",
    "\n",
    "            q_values, terminated, reward, action, target_q_values = rb_workspace[\n",
    "                \"q_values\", \"env/terminated\", \"env/reward\", \"action\", \"target/q_values\"\n",
    "            ]\n",
    "\n",
    "            # Determines whether values of the critic should be propagated\n",
    "            must_bootstrap = ~terminated\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = compute_critic_loss(\n",
    "                dqn.cfg, reward, must_bootstrap, q_values, target_q_values, action\n",
    "            )\n",
    "            # Store the loss for tensorboard display\n",
    "            dqn.logger.add_log(\"critic_loss\", critic_loss, dqn.nb_steps)\n",
    "            dqn.logger.add_log(\n",
    "                \"q_values/min\", q_values.max(-1).values.min(), dqn.nb_steps\n",
    "            )\n",
    "            dqn.logger.add_log(\n",
    "                \"q_values/max\", q_values.max(-1).values.max(), dqn.nb_steps\n",
    "            )\n",
    "            dqn.logger.add_log(\n",
    "                \"q_values/mean\", q_values.max(-1).values.mean(), dqn.nb_steps\n",
    "            )\n",
    "\n",
    "            dqn.optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                dqn.t_q_agent.parameters(), dqn.cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            dqn.optimizer.step()\n",
    "\n",
    "            # Update target\n",
    "            if (\n",
    "                dqn.nb_steps - dqn.last_critic_update_step\n",
    "                > dqn.cfg.algorithm.target_critic_update\n",
    "            ):\n",
    "                dqn.last_critic_update_step = dqn.nb_steps\n",
    "                copy_parameters(dqn.t_q_agent, dqn.t_target_q_agent)\n",
    "\n",
    "            dqn.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff3c158",
   "metadata": {},
   "source": [
    "## Definition of the parameters\n",
    "\n",
    "The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a\n",
    "tensorboard visualisation.\n",
    "\n",
    "### Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b3f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbcdef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"base_dir\": \"${gym_env.env_name}/dqn-S${algorithm.seed}_${current_time:}\",\n",
    "    # `collect_stats` is True: we keep the cumulated reward for all\n",
    "    # evaluation episodes\n",
    "    \"collect_stats\": True,\n",
    "    \"save_best\": False,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 4,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"epsilon\": 0.02,\n",
    "        \"n_envs\": 8,\n",
    "        \"n_steps\": 32,\n",
    "        \"n_updates\": 32,\n",
    "        \"eval_interval\": 2000,\n",
    "        \"learning_starts\": 5000,\n",
    "        \"nb_evals\": 10,\n",
    "        \"buffer_size\": 100_000,\n",
    "        \"batch_size\": 256,\n",
    "        \"target_critic_update\": 1_000,\n",
    "        \"max_epochs\": 3_000,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"architecture\": {\"hidden_size\": [256, 256]},\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPole-v1\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "}\n",
    "\n",
    "dqn = DQN(OmegaConf.create(params))\n",
    "run(dqn, dqn_compute_critic_loss)\n",
    "dqn.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e30a0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Coding Exercise: Double DQN (DDQN)\n",
    "\n",
    "In DQN, the same network is responsible for selecting and estimating the best\n",
    "next action (in the TD-target) and that may lead to over-estimation: the\n",
    "action which q-value is over-estimated will be chosen more often. As a result,\n",
    "training is slower.\n",
    "\n",
    "To reduce over-estimation, double q-learning (and then DDQN) was proposed. It\n",
    "decouples the action selection from the value estimation.\n",
    "\n",
    "Concretely, in DQN, the target value in the critic loss (used to update the Q\n",
    "critic) for a sample at time $t$ is defined as:\n",
    "\n",
    "$$Y^{DQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1},\n",
    "a; \\mathbb{\\theta}_{target}\\right); \\mathbb{\\theta}_{target}\\right)$$\n",
    "\n",
    "where the target network `target_q_agent` with parameters\n",
    "$\\mathbb{\\theta}_{target}$ is used for both action selection and estimation,\n",
    "and can therefore be rewritten:\n",
    "\n",
    "$$Y^{DQN}_{t} = r_{t+1} + \\gamma \\max_{a}{Q}\\left(s_{t+1}, a;\n",
    "\\mathbb{\\theta}_{target}\\right)$$\n",
    "\n",
    "Instead, DDQN uses the online critic `q_agent` with parameters\n",
    "$\\mathbb{\\theta}_{online}$ to select the action, whereas it uses the target\n",
    "network `target_q_agent` to estimate the associated Q-values:\n",
    "\n",
    "$$Y^{DDQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1},\n",
    "a; \\mathbb{\\theta}_{online}\\right); \\mathbb{\\theta}_{target}\\right)$$\n",
    "\n",
    "The goal in this exercise is for you to write the update method for `DDQN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dbd80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddqn_compute_critic_loss(\n",
    "    cfg, reward, must_bootstrap, q_values, target_q_values, action\n",
    "):\n",
    "    \"\"\"Compute the critic loss\n",
    "\n",
    "    :param reward: The reward $r_t$ (shape 2 x B)\n",
    "    :param must_bootstrap: The must bootstrap flag at $t+1$ (shape 2 x B)\n",
    "    :param q_values: The Q-values (shape 2 x B x A)\n",
    "    :param target_q_values: The target Q-values (shape 2 x B x A)\n",
    "    :param action: The chosen actions (shape 2 x B)\n",
    "    :return: the loss (a scalar)\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement the double DQN loss\n",
    "\n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "    # Compute critic loss\n",
    "    mse = nn.MSELoss()\n",
    "    critic_loss = mse(target, qvals)\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ffb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"base_dir\": \"${gym_env.env_name}/double-dqn-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    \"save_best\": False,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 3,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"epsilon\": 0.02,\n",
    "        \"n_envs\": 8,\n",
    "        \"n_steps\": 32,\n",
    "        \"n_updates\": 32,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"learning_starts\": 5_000,\n",
    "        \"nb_evals\": 10,\n",
    "        \"buffer_size\": 100_000,\n",
    "        \"batch_size\": 256,\n",
    "        \"target_critic_update\": 1000,\n",
    "        \"max_epochs\": 3_000,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"architecture\": {\"hidden_size\": [128, 128]},\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPole-v1\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "}\n",
    "\n",
    "ddqn = DQN(OmegaConf.create(params))\n",
    "run(ddqn, ddqn_compute_critic_loss)\n",
    "ddqn.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c598a4b",
   "metadata": {},
   "source": [
    "## Comparing DQN and D-DQN\n",
    "\n",
    "Finally, we compare DQN and double DQN, computing a Welch t-test\n",
    "for every evaluation (the dot above the curves marks a significant\n",
    "difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f299e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbrl.stats import WelchTTest\n",
    "\n",
    "WelchTTest().plot(\n",
    "    torch.stack(dqn.eval_rewards), torch.stack(ddqn.eval_rewards), save=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
