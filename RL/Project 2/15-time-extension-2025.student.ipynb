{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45c07bf",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, using BBRL, you will study some effects of partial observability\n",
    "on the continuous action version of the LunarLander-v3 environment, using the TD3 algorithm.\n",
    "\n",
    "To emulate partial observability, you will design dedicated wrappers. Then you will study\n",
    "whether extending the input of the agent policy and critic with a memory of previous states\n",
    "and can help solve the partial observability issue. Yu will also study whether using action chunks\n",
    "instead of single actions as ouptput has an effect of the learning performance.\n",
    "This will also be achieved by designing other temporal extension wrappers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8fac0",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45cc1c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Prepare the environment\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.visu.plot_policies import plot_policy\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import bbrl_utils\n",
    "\n",
    "bbrl_utils.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff04b0",
   "metadata": {},
   "source": [
    "# Temporal modification wrappers\n",
    "\n",
    "The [LunarLander-v3](https://gymnasium.farama.org/environments/box2d/lunar_lander/) environment is a gymnasium environment.\n",
    "See the gymnasium page for a description of the state and action spaces.\n",
    "\n",
    "To emulate partial observability in LunarLander-v3, you will hide the x and y velocities of the lander\n",
    "by filtering them out of the state returned by the environment.\n",
    "This is implemented with the ```FeatureFilterWrapper```.\n",
    "\n",
    "To compensate for partial observability, you will extend the architecture of the agent\n",
    "with a memory of previous states and extend its output with action chunks.\n",
    "This is implemented with two wrappers, the ```ObsTimeExtensionWrapper```\n",
    "and the ```ActionTimeExtensionWrapper```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae02b8",
   "metadata": {},
   "source": [
    "## The FeatureFilterWrapper\n",
    "\n",
    "The FeatureFilterWrapper removes a feature from the returned observation\n",
    "when calling the ```reset()``` and ```step(action)``` functions.\n",
    "The index of the removed feature is given as a parameter when building the object.\n",
    "\n",
    "To filter out the x and y velocities from the LunarLander-v3 environment,\n",
    "the idea is to call the wrapper the adequate number of times, using something like\n",
    "```env = FeatureFilterWrapper(FeatureFilterWrapper(inner_env, X), Y)```\n",
    "where ```inner_env``` is the LunarLander-v3 environment\n",
    "and X and Y are position of features you want to filter out.\n",
    "\n",
    "Beware of filtering features in the right order,\n",
    "as removing a feature changes the index of all subsequent features.\n",
    "\n",
    "One way to put such a wrapper with a parameter into the list of wrappers is to use a lambda:\n",
    "for instance, `lambda env: FeatureFilterWrapper(env, 3),`\n",
    "\n",
    "### Exercise 1: code the FeatureFilterWrapper class below.\n",
    "\n",
    "Beyond rewriting the ```reset()``` and ```step(action)``` functions,\n",
    "beware of adapting the observation space and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e16483",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# [[STUDENT]]...\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479234fe",
   "metadata": {},
   "source": [
    "## The ObsTimeExtensionWrapper\n",
    "\n",
    "When facing a partially observable environment, training with RL a reactive agent which just selects an action based on the current observation\n",
    "is not guaranteed to reach optimality. An option to mitigate this fundamental limitation is to equip the agent with a memory of the past.\n",
    "\n",
    "One way to do so is to use a recurrent neural network instead of a feedforward one to implement the agent: the neural network contains\n",
    "some memory capacity and the RL process may tune this internal memory so as to remember exactly what is necessary from the\n",
    "past observation. This has been done many times using an LSTM, see for instance\n",
    "[this early paper](https://proceedings.neurips.cc/paper/2001/file/a38b16173474ba8b1a95bcbc30d3b8a5-Paper.pdf).\n",
    "\n",
    "Another way to do so is to equip the agent with a list-like memory of the past observations\n",
    "and to extend the critic and policy to take as input the current observation and the previous ones.\n",
    "This removes the difficulty of learning an adequate representation of the past, but this results in\n",
    "enlarging the input size of the actor and critic networks. This can only be done if the required memory\n",
    "horizon to behave optimally is small enough.\n",
    "\n",
    "In the case of the LunarLander-v3 environment, one can immediately see that a memory of the previous\n",
    "x and y coordinates is enough to compensate for the absence of the velocities,\n",
    "since $\\dot{a} \\approx (a_{t} - a_{t-1})$.\n",
    "\n",
    "So we will extend the RL agent with a memory of size 1.\n",
    "\n",
    "Though it may not be intuitive at first glance, the simplest way to do so is to embed the environment\n",
    "into a wrapper which contains the required memory and produces the extended observations.\n",
    "This way, the RL agent will naturally be built with an extended observation space,\n",
    "and the wrapper will be in charge of concatenating the memorized observation from the previous step\n",
    "with the current observation received from the inner environment when calling the ```step(action)``` function.\n",
    "When calling the ```reset()``` function, the memory of observations should be reinitialized with null observations.\n",
    "\n",
    "### Exercise 2: code the ObsTimeExtensionWrapper class below.\n",
    "\n",
    "Beyond rewriting the ```reset()``` and ```step(action)``` functions, beware of adapting the observation space and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[STUDENT]]...\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed0ba7",
   "metadata": {},
   "source": [
    "## The ActionTimeExtensionWrapper\n",
    "\n",
    "It has been observed that, in partially observable environments, preparing to play\n",
    "a sequence of actions and only playing the first can be better than only preparing for one action.\n",
    "The difference comes from the fact that the critic evaluates\n",
    "sequences of actions, even if only the first is played in practice.\n",
    "\n",
    "Similarly to the ObsTimeExtensionWrapper, the corresponding behavior can be implemented with a wrapper.\n",
    "The size of the action space of the extended environment should be\n",
    "M times the size of the action space of the inner environment. This ensures that the policy and the critic\n",
    "will consider extended actions.\n",
    "Besides, the ```step(action)``` function should receive an extended actions of size M times\n",
    "the size of an action, and should only transmit the first action to the inner environment.\n",
    "\n",
    "Warning, in gymnasium the case where the action is one dimensional requires a slightly\n",
    "different treatment with respect to when it is multi-dimensional\n",
    "\n",
    "### Exercise 3: code the ActionTimeExtensionWrapper class below.\n",
    "\n",
    "Beyond rewriting the ```reset()``` and ```step(action)``` functions, beware of adapting the action space and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616671d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# [[STUDENT]]...\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1f5a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TD3(EpochBasedAlgo):\n",
    "    def __init__(self, cfg, wrappers_factory):\n",
    "        super().__init__(cfg, wrappers_factory)\n",
    "\n",
    "        # Define the agents and optimizers for TD3\n",
    "\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_td3(td3: TD3):\n",
    "    for rb in td3.iter_replay_buffers():\n",
    "        rb_workspace = rb.get_shuffled(td3.cfg.algorithm.batch_size)\n",
    "\n",
    "        # Implement the learning loop\n",
    "\n",
    "        assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a1375f",
   "metadata": {},
   "source": [
    "## Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0fb8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_tensorboard(\"./outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7a98e",
   "metadata": {},
   "source": [
    "# Experimental study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51793a6e",
   "metadata": {},
   "source": [
    "To run the experiments below, you can use the [TD3](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf) algorithm.\n",
    "\n",
    "You can just copy paste here the code you have used during the corresponding labs.\n",
    "We only provide a suggested set of hyper-parameters working well on the LunarLander-v3 environment for TD3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614674fa",
   "metadata": {},
   "source": [
    "## Definition of the parameters\n",
    "\n",
    "The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a\n",
    "tensorboard visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"save_best\": False,\n",
    "    \"base_dir\": \"${gym_env.env_name}/td3-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": False,\n",
    "    # Set to true to have an insight on the learned policy\n",
    "    # (but slows down the evaluation a lot!)\n",
    "    \"plot_agents\": False,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 2,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"n_envs\": 1,\n",
    "        \"n_steps\": 1000,\n",
    "        \"nb_evals\": 10,\n",
    "        \"discount_factor\": 0.99999,\n",
    "        \"buffer_size\": 1e6,\n",
    "        \"batch_size\": 256,\n",
    "        \"tau_target\": 0.005,\n",
    "        \"eval_interval\": 5_000,\n",
    "        \"max_epochs\": 3500,\n",
    "        # Minimum number of transitions before learning starts\n",
    "        \"learning_starts\": 100,\n",
    "        \"action_noise\": 0.2,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [64, 64],\n",
    "            \"critic_hidden_size\": [400, 300],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"LunarLander-v3\",\n",
    "        \"env_args\": { \"continuous\": True, }\n",
    "    },\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"eps\": 5e-5,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"eps\": 5e-5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76830dc3",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "You know have all the elements to study the impact of removing features from the environment\n",
    "on the training performance, and the impact of temporally extending the agent in mitigating\n",
    "partial observability, both with observation and with action extension.\n",
    "\n",
    "In practice, you should produce the following learning curves:\n",
    "\n",
    "- a learning curve of your algorithm on the standard LunarLander-v3 environment with full observability,\n",
    "- two learning curves, one from removing $\\dot{x}$ from LunarLander-v3 and the other from removing $\\dot{\\theta}$,\n",
    "- one learning curve from removing both $\\dot{x}$ and $\\dot{\\theta}$,\n",
    "- the same four learning curves as above, but adding each of the temporal extension wrappers, separately or combined.\n",
    "\n",
    "The way to combine these learning curves in different figures is open to you but should be carefully considered\n",
    "depending on the conclusions you want to draw. Beware of drawing conclusions from insufficient statistics.\n",
    "\n",
    "Discuss what you observe and conclude from this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7cd53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[STUDENT]]...\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee54e99",
   "metadata": {},
   "source": [
    "# Lab report\n",
    "\n",
    "Your report should contain:\n",
    "- your source code (probably this notebook), do not forget to put your names on top of the notebook,\n",
    "- in a separate pdf file with your names in the name of the file (name1_name2.pdf),  no longer than 6 pages:\n",
    "    + a detailed enough description of all the choices you have made: the parameters you have set, the algorithms you have used, etc.,\n",
    "    + the curves obtained when doing Exercise 3,\n",
    "    + your conclusion from these experiments.\n",
    "\n",
    "Beyond the elements required in this report, any additional study will be rewarded.\n",
    "For instance, you can extend the temporal horizon for the state memory and or action sequences beyond 2,\n",
    "and study the impact on learning performance and training time, etc.\n",
    "A great achievement would be to perform a comparison with the approach based on an LSTM."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
