{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dedf8f67",
   "metadata": {},
   "source": [
    " Copyright © Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the\n",
    " LICENSE file in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d9232e",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook we will study basic reinforcement learning\n",
    "algorithms: TD learning, Q-learning and SARSA. We will also investigate two\n",
    "basic exploration strategies: $\\epsilon$-greedy and softmax.\n",
    "\n",
    "\n",
    "## Initialization\n",
    "\n",
    "We begin by loading all the modules necessary for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ac57ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib backend: module://matplotlib_inline.backend_inline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\titouan\\OneDrive\\Bureau\\M2-MIND\\RL\\venv\\Lib\\site-packages\\bbrl_utils\\notebook.py:46: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # noqa: F401\n"
     ]
    }
   ],
   "source": [
    "from mazemdp import random_policy\n",
    "from mazemdp.toolbox import egreedy, egreedy_loc, sample_categorical, softmax\n",
    "from mazemdp.mdp import Mdp  # noqa: F401\n",
    "from bbrl_utils.notebook import tqdm\n",
    "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "\n",
    "\n",
    "import bbrl_utils\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
    "from bbrl_utils.notebook import tqdm\n",
    "from mazemdp.mdp import Mdp  # noqa: F401\n",
    "from mazemdp.toolbox import egreedy, egreedy_loc, sample_categorical, softmax\n",
    "from mazemdp import random_policy\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "\n",
    "bbrl_utils.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752ee27",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning is about finding the optimal policy in an MDP which is\n",
    "initially unknown to the agent. More precisely, the state and action spaces\n",
    "are known, but the agent does not know the transition and reward functions.\n",
    "Generally speaking, the agent has to explore the MDP to figure out which\n",
    "action in which state leads to which other state and reward. The model-free\n",
    "case is about finding this optimal policy just through very local updates,\n",
    "without storing any information about previous interactions with the\n",
    "environment. Principles of these local updates can already be found in the\n",
    "Temporal Difference (TD) algorithm, which iteratively computes optimal values\n",
    "for all state using local updates. The most widely used model-free RL\n",
    "algorithms are **q-learning**, **SARSA** and **actor-critic** algorithms.\n",
    "\n",
    "As for dynamic programming, we first create a maze-like MDP. Reinforcement\n",
    "learning is slower than dynamic programming, so we will work with smaller\n",
    "mazes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94382cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table.maze {\n",
       "    border-collapse: collapse;\n",
       "}\n",
       "\n",
       "td {\n",
       "    text-align: center;\n",
       "}\n",
       "\n",
       "table.maze td.cell {\n",
       "    position: relative;\n",
       "    border: 1px solid black;\n",
       "}\n",
       "\n",
       "table.maze td.cell div.agent {\n",
       "    position: absolute;\n",
       "    left: 25%;\n",
       "    top: 25%;;\n",
       "    width: 50%;\n",
       "    height: 50%;\n",
       "    border-radius: 100%;\n",
       "    background:rgba(69, 100, 186, 0.5);\n",
       "}\n",
       "\n",
       "\n",
       "td.wall {\n",
       "    background: black;\n",
       "}\n",
       "\n",
       "td.terminal {\n",
       "    background: rgb(246, 170, 246);\n",
       "}\n",
       "\n",
       "table.maze table td {\n",
       "    width: .5rem;\n",
       "    height: .5rem;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "table.maze table td.arrow {\n",
       "    color: transparent;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       "table.maze table td.value {\n",
       "    position: relative;\n",
       "    width: 2rem;\n",
       "    height: 2rem;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3787de7b3b4786809cd992f9c115b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MazeWidget(cells=array([[ 0,  2,  5,  7],\n",
       "       [-1,  3,  6,  8],\n",
       "       [ 1,  4, -1,  9]]), t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Environment with 20% of walls and no negative reward when hitting a wall\n",
    "env = gym.make(\n",
    "    \"MazeMDP-v0\",\n",
    "    kwargs={\"width\": 4, \"height\": 3, \"ratio\": 0.2, \"hit\": 0.0, \"start_states\": [0]},\n",
    "    render_mode=\"human\",\n",
    ")\n",
    "env = env.unwrapped  # the .unwrapped removes a warning from gymnasium\n",
    "env.reset()\n",
    "env.init_draw(\"The maze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425ca20",
   "metadata": {},
   "source": [
    "# Temporal Difference (TD) learning ##\n",
    "\n",
    "Given a state and an action spaces as well as a policy, TD(0) computes the\n",
    "state value of this policy based on the following equations:\n",
    "\n",
    "$$\\delta_t = r(s_t,a_t) + \\gamma V^{(i)}(s_{t+1})-V^{(i)}(s_t)$$\n",
    "$$V^{(i+1)}(s_t) = V^{(i)}(s_t) + \\alpha\\delta_t$$\n",
    "\n",
    "where $\\delta$ is the TD error and $\\alpha$ is a parameter called \"learning\n",
    "rate\".\n",
    "\n",
    "Note however that when the episode terminates in state $s_t$,\n",
    "back-propagating the value of $s_{t+1}$ makes no sense,\n",
    "as the environment will be reset for the next episode.\n",
    "So, in such a case, we should ignore the $\\gamma V^{(i)}(s_{t+1})$ term.\n",
    "We could write this with if terminated: $\\delta_t = r(s_t,a_t) -V^{(i)}(s_t)$ else standard update,\n",
    "but since the `terminated` boolean can be seen as an integer,\n",
    "we can obtain the same behaviour with $\\delta_t = r(s_t,a_t) + \\gamma V^{(i)}(s_{t+1}) (1 - terminated) - V^{(i)}(s_t)$.\n",
    "\n",
    "The code is provided below, so that you can take inspiration later on. The\n",
    "important part is the computation of $\\delta$, and the update of the values of\n",
    "$V$.\n",
    "\n",
    "To run TD learning, a policy is needed as input. Such a policy can be\n",
    "retreived by using the `policy_iteration_q(mdp)` function defined in the\n",
    "dynamic programming notebook.\n",
    "\n",
    "If you want to run this notebook independently, you can use instead the\n",
    "`random_policy` provided in `mazemdp`. This is what we do here by default,\n",
    "replace it if you want to run TD learning from an optimal policy.\n",
    "\n",
    "The ```evaluate``` function below is not necessary for the lab, it is left here for its informative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e86e3a15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def evaluate(mdp, policy):\n",
    "    x, _ = env.reset(options={\"uniform\": True})\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    reward = 0\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        # Perform a step of the MDP\n",
    "        u = sample_categorical(policy[x])\n",
    "        _, r, terminated, truncated, *_ = env.step(u)\n",
    "        reward += r\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b73179a",
   "metadata": {},
   "source": [
    "**Question:** In the code of the *temporal_difference(...)* function below,\n",
    "fill the missing parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "894cc1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference(\n",
    "    env: MazeMDPEnv,\n",
    "    policy: np.ndarray,\n",
    "    nb_episodes: int = 50,\n",
    "    alpha: float = 0.2,\n",
    "    render: bool = True,\n",
    ") -> np.ndarray:\n",
    "    mdp = env.unwrapped\n",
    "\n",
    "    # alpha: learning rate\n",
    "    v = np.zeros(mdp.unwrapped.nb_states)  # initial state value v\n",
    "\n",
    "    if render:\n",
    "        mdp.init_draw(\"Temporal differences\")\n",
    "\n",
    "    for _ in tqdm(range(nb_episodes)):  # for each episode\n",
    "        # Draw an initial state randomly (if uniform is set to False, the state\n",
    "        # is drawn according to the P0 distribution)\n",
    "        x, _ = env.reset(options={\"uniform\": True})\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        while not (terminated or truncated):\n",
    "            # Show agent\n",
    "            if render:\n",
    "                mdp.draw_v_pi(v, policy)\n",
    "\n",
    "            # Step forward following the MDP: s=current state, pol[i]=agent's\n",
    "            # action according to policy pol, r=reward gained after taking\n",
    "            # action pol[i], terminated=tells whether  the episode ended, and info\n",
    "            # gives some info about the process\n",
    "            y, r, terminated, truncated, _ = env.step(\n",
    "                egreedy_loc(policy[x], mdp.action_space.n, epsilon=0.2)\n",
    "            )\n",
    "            # [[STUDENT]]...\n",
    "\n",
    "            # Update the state value of x\n",
    "            delta = r + mdp.gamma * v[y]*(1-terminated) - v[x]\n",
    "            v[x] = v[x] + alpha * delta\n",
    "\n",
    "            # Update agent's position (state)\n",
    "            s = y\n",
    "\n",
    "    if render:\n",
    "        mdp.current_state = 0\n",
    "        mdp.draw_v_pi(v, policy)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca2443",
   "metadata": {},
   "source": [
    "Once this is done, you can run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e912f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table.maze {\n",
       "    border-collapse: collapse;\n",
       "}\n",
       "\n",
       "td {\n",
       "    text-align: center;\n",
       "}\n",
       "\n",
       "table.maze td.cell {\n",
       "    position: relative;\n",
       "    border: 1px solid black;\n",
       "}\n",
       "\n",
       "table.maze td.cell div.agent {\n",
       "    position: absolute;\n",
       "    left: 25%;\n",
       "    top: 25%;;\n",
       "    width: 50%;\n",
       "    height: 50%;\n",
       "    border-radius: 100%;\n",
       "    background:rgba(69, 100, 186, 0.5);\n",
       "}\n",
       "\n",
       "\n",
       "td.wall {\n",
       "    background: black;\n",
       "}\n",
       "\n",
       "td.terminal {\n",
       "    background: rgb(246, 170, 246);\n",
       "}\n",
       "\n",
       "table.maze table td {\n",
       "    width: .5rem;\n",
       "    height: .5rem;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "table.maze table td.arrow {\n",
       "    color: transparent;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       "table.maze table td.value {\n",
       "    position: relative;\n",
       "    width: 2rem;\n",
       "    height: 2rem;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310cae3ca0e747709b12c21e680d60aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MazeWidget(cells=array([[ 0,  3, -1,  7],\n",
       "       [ 1,  4,  5,  8],\n",
       "       [ 2, -1,  6,  9]]), t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9fa8d8fd314fc4be01c0b1ad766294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy = random_policy(env.unwrapped)\n",
    "v = temporal_difference(env, policy, nb_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcb0dd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Unless you were lucky, the generated value function is boring: if the policy\n",
    "does not reach the final state, all values are 0. To avoid this, you can\n",
    "copy-paste a dynamic programming function on the Q function from the previous\n",
    "notebook, use it to get an optimal policy, and use this policy for TD\n",
    "learning. You should get a much more interesting value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897fe92a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Put your code to obtain an optimal Q function here\n",
    "def evaluate_one_step_q(\n",
    "    mdp: MazeMDPEnv, q: np.ndarray, policy: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    # Outputs the state value function after one step of policy evaluation\n",
    "    qnew = np.zeros(\n",
    "        (mdp.nb_states, mdp.action_space.n)\n",
    "    )  # initial action values are set to 0\n",
    "    evaluat_q_update = 0\n",
    "    for x in range(mdp.nb_states):  # for each state x\n",
    "        # Compute the value of the state x for each action u of the MDP action space\n",
    "        for u in range(mdp.action_space.n):\n",
    "            if x in mdp.terminal_states:\n",
    "                qnew[x, u] = mdp.r[x, u]\n",
    "                evaluat_q_update += 1\n",
    "            else:\n",
    "                # Process sum of the values of the neighbouring states\n",
    "                summ = 0\n",
    "                for y in range(mdp.nb_states):\n",
    "                    # [[STUDENT]]...\n",
    "\n",
    "                    summ += mdp.P[x, u, y] * q[y, policy[y]]\n",
    "\n",
    "\n",
    "                # [[STUDENT]]...\n",
    "\n",
    "                qnew[x, u] = mdp.r[x, u] + mdp.gamma * summ\n",
    "                evaluat_q_update += 1\n",
    "    # print(f\"Number Q evaluate updates : {evaluat_q_update}\")\n",
    "\n",
    "    return qnew\n",
    "\n",
    "def evaluate_q(mdp: MazeMDPEnv, policy: np.ndarray) -> np.ndarray:\n",
    "    # Outputs the state value function of a policy\n",
    "    q = np.zeros(\n",
    "        (mdp.nb_states, mdp.action_space.n)\n",
    "    )  # initial action values are set to 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        qold = q.copy()\n",
    "\n",
    "        # [[STUDENT]]...\n",
    "\n",
    "        q = evaluate_one_step_q(mdp, qold, policy)\n",
    "\n",
    "\n",
    "        # Test if convergence has been reached\n",
    "        if (np.linalg.norm(q - qold)) < 0.01:\n",
    "            stop = True\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63d0223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code to get a policy from a Q function here\n",
    "def get_policy_from_q(q: np.ndarray) -> np.ndarray:\n",
    "    # Outputs a policy given the action values\n",
    "    policy = np.zeros(q.shape[0], dtype=int)\n",
    "    for x in range(q.shape[0]):\n",
    "        policy[x] = np.argmax(q[x, :])\n",
    "    return policy\n",
    "\n",
    "# ---------------- Policy Iteration with the Q function -----------------#\n",
    "# Given a MDP, this algorithm simultaneously computes\n",
    "# the optimal action value function Q and the optimal policy\n",
    "\n",
    "\n",
    "def policy_iteration_q(\n",
    "    mdp: MazeMDPEnv, render: bool = True\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"policy iteration over the q function.\"\"\"\n",
    "    q = np.zeros(\n",
    "        (mdp.nb_states, mdp.action_space.n)\n",
    "    )  # initial action values are set to 0\n",
    "    q_list = []\n",
    "    policy = random_policy(mdp)\n",
    "\n",
    "    stop = False\n",
    "\n",
    "    mdp.init_draw(\"Policy iteration Q\")\n",
    "\n",
    "    while not stop:\n",
    "        qold = q.copy()\n",
    "        mdp.draw_v(q)\n",
    "\n",
    "        # Step 1 : Policy evaluation\n",
    "\n",
    "        q = evaluate_q(mdp, policy)\n",
    "\n",
    "\n",
    "        # Step 2 : Policy improvement\n",
    "\n",
    "        policy = get_policy_from_q(q)\n",
    "\n",
    "\n",
    "        # Check convergence\n",
    "        if (np.linalg.norm(q - qold)) <= 0.01:\n",
    "            stop = True\n",
    "        q_list.append(np.linalg.norm(q))\n",
    "\n",
    "    mdp.draw_v_pi(q, get_policy_from_q(q))\n",
    "    return q, q_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4289fa32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table.maze {\n",
       "    border-collapse: collapse;\n",
       "}\n",
       "\n",
       "td {\n",
       "    text-align: center;\n",
       "}\n",
       "\n",
       "table.maze td.cell {\n",
       "    position: relative;\n",
       "    border: 1px solid black;\n",
       "}\n",
       "\n",
       "table.maze td.cell div.agent {\n",
       "    position: absolute;\n",
       "    left: 25%;\n",
       "    top: 25%;;\n",
       "    width: 50%;\n",
       "    height: 50%;\n",
       "    border-radius: 100%;\n",
       "    background:rgba(69, 100, 186, 0.5);\n",
       "}\n",
       "\n",
       "\n",
       "td.wall {\n",
       "    background: black;\n",
       "}\n",
       "\n",
       "td.terminal {\n",
       "    background: rgb(246, 170, 246);\n",
       "}\n",
       "\n",
       "table.maze table td {\n",
       "    width: .5rem;\n",
       "    height: .5rem;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "table.maze table td.arrow {\n",
       "    color: transparent;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       "table.maze table td.value {\n",
       "    position: relative;\n",
       "    width: 2rem;\n",
       "    height: 2rem;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526cea6e82db437396a1d6a922a5cc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MazeWidget(cells=array([[ 0,  2,  5,  7],\n",
       "       [-1,  3,  6,  8],\n",
       "       [ 1,  4, -1,  9]]), t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Put your code to run the algorithm here\n",
    "q, q_list = policy_iteration_q(env.unwrapped, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e3261",
   "metadata": {},
   "source": [
    "# Q-learning ##\n",
    "\n",
    "The **Q-learning** algorithm accounts for an agent exploring an MDP and\n",
    "updating at each step a model of the state action-value function stored into a\n",
    "Q-table. It is updated as follows:\n",
    "\n",
    "$$\n",
    "\\delta_t = \\left( r(s_t,a_t) + \\gamma \\max_{a \\in A}\n",
    "Q^{(i)}(s_{t+1},a) \\right) -Q^{(i)}(s_t,a_t)\n",
    "$$\n",
    "\n",
    "$$Q^{(i+1)}(s_t, a_t) = Q^{(i)}(s_t,a_t) + \\alpha \\delta_t$$\n",
    "\n",
    "To visualize the policy, we need the `get_policy_from_q(q)` function that we defined in the\n",
    "dynamic programming notebook. If you have not done so yet, import it below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932973d1",
   "metadata": {},
   "source": [
    "Fill the code of the `q_learning(...)` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e69ebc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Q-Learning epsilon-greedy version -------------------------------#\n",
    "# Given an exploration rate epsilon, the QLearning algorithm computes the state action-value function\n",
    "# based on an epsilon-greedy policy\n",
    "# alpha is the learning rate\n",
    "\n",
    "\n",
    "def q_learning_eps(\n",
    "    mdp: MazeMDPEnv,\n",
    "    epsilon: float,\n",
    "    learning_rate: float,\n",
    "    nb_episodes: int = 20,\n",
    "    render: bool = True,\n",
    "    init_q: float = 0.0,\n",
    "    uniform: bool = True,\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    mdp = env.unwrapped\n",
    "\n",
    "    # Initialize the state-action value function\n",
    "    # alpha is the learning rate\n",
    "    q = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_min = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q[:, :] = init_q\n",
    "    q_list = []\n",
    "    time_list = []\n",
    "\n",
    "    # Run learning cycle\n",
    "    mdp = env.unwrapped\n",
    "\n",
    "    if render:\n",
    "        mdp.init_draw(\"Q Learning\")\n",
    "\n",
    "    for _ in range(nb_episodes):\n",
    "        # Draw the first state of episode i using a uniform distribution over all the states\n",
    "        s, _ = env.reset(options={\"uniform\": uniform})\n",
    "        cpt = 0\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        while not (terminated or truncated):\n",
    "            # Show the agent in the maze\n",
    "            if render:\n",
    "                mdp.draw_v_pi(q, q.argmax(axis=1))\n",
    "\n",
    "            # Draw an action using an epsilon-greedy policy\n",
    "            a = egreedy(q, s, epsilon)\n",
    "\n",
    "            # Perform a step of the MDP\n",
    "            s_next, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "            # [[STUDENT]]...\n",
    "\n",
    "            # Update the state-action value function with q-Learning\n",
    "            delta = (r + mdp.gamma * (1-terminated) * np.max(q[s_next, :])) - q[s, a]\n",
    "            q[s,a] = q[s,a] + learning_rate * delta\n",
    "\n",
    "            # Update the agent position\n",
    "            s = s_next\n",
    "            cpt = cpt + 1\n",
    "\n",
    "        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n",
    "        time_list.append(cpt)\n",
    "\n",
    "    if render:\n",
    "        mdp.current_state = 0\n",
    "        mdp.draw_v_pi(q, get_policy_from_q(q))\n",
    "\n",
    "    return q, q_list, time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ffdc0",
   "metadata": {},
   "source": [
    "And run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "052ed17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table.maze {\n",
       "    border-collapse: collapse;\n",
       "}\n",
       "\n",
       "td {\n",
       "    text-align: center;\n",
       "}\n",
       "\n",
       "table.maze td.cell {\n",
       "    position: relative;\n",
       "    border: 1px solid black;\n",
       "}\n",
       "\n",
       "table.maze td.cell div.agent {\n",
       "    position: absolute;\n",
       "    left: 25%;\n",
       "    top: 25%;;\n",
       "    width: 50%;\n",
       "    height: 50%;\n",
       "    border-radius: 100%;\n",
       "    background:rgba(69, 100, 186, 0.5);\n",
       "}\n",
       "\n",
       "\n",
       "td.wall {\n",
       "    background: black;\n",
       "}\n",
       "\n",
       "td.terminal {\n",
       "    background: rgb(246, 170, 246);\n",
       "}\n",
       "\n",
       "table.maze table td {\n",
       "    width: .5rem;\n",
       "    height: .5rem;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "table.maze table td.arrow {\n",
       "    color: transparent;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       "table.maze table td.value {\n",
       "    position: relative;\n",
       "    width: 2rem;\n",
       "    height: 2rem;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d767873fa1a4267b39646156e60d193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MazeWidget(cells=array([[ 0,  2,  5,  7],\n",
       "       [-1,  3,  6,  8],\n",
       "       [ 1,  4, -1,  9]]), t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.5\n",
    "epsilon = 0.02\n",
    "MAX_EPISODES = 200\n",
    "q, q_list, time_list = q_learning_eps(env, epsilon, learning_rate, nb_episodes=MAX_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25b857",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Harder case: fixed starting point and exploration\n",
    "\n",
    "We now explore the case where the agent always start at the *beginning of the maze* (`uniform=False`), corresponding to the top-left corner when this is a free cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1508db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table.maze {\n",
       "    border-collapse: collapse;\n",
       "}\n",
       "\n",
       "td {\n",
       "    text-align: center;\n",
       "}\n",
       "\n",
       "table.maze td.cell {\n",
       "    position: relative;\n",
       "    border: 1px solid black;\n",
       "}\n",
       "\n",
       "table.maze td.cell div.agent {\n",
       "    position: absolute;\n",
       "    left: 25%;\n",
       "    top: 25%;;\n",
       "    width: 50%;\n",
       "    height: 50%;\n",
       "    border-radius: 100%;\n",
       "    background:rgba(69, 100, 186, 0.5);\n",
       "}\n",
       "\n",
       "\n",
       "td.wall {\n",
       "    background: black;\n",
       "}\n",
       "\n",
       "td.terminal {\n",
       "    background: rgb(246, 170, 246);\n",
       "}\n",
       "\n",
       "table.maze table td {\n",
       "    width: .5rem;\n",
       "    height: .5rem;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "table.maze table td.arrow {\n",
       "    color: transparent;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       "table.maze table td.value {\n",
       "    position: relative;\n",
       "    width: 2rem;\n",
       "    height: 2rem;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ada1285365490784c190c1c076ae91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MazeWidget(cells=array([[ 0,  2,  5,  7],\n",
       "       [-1,  3,  6,  8],\n",
       "       [ 1,  4, -1,  9]]), t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon = 0.02\n",
    "start_q, start_q_list, time_list = q_learning_eps(\n",
    "    env, epsilon, learning_rate=0.5, nb_episodes=MAX_EPISODES, uniform=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f094c21b",
   "metadata": {},
   "source": [
    "You will observe that it is very difficult for the agent to learn to reach the\n",
    "final state (and the larger the maze, the more difficult). A simple trick to\n",
    "avoid this is to initialize the value of each $(s,a)$ pair to a small (lower\n",
    "than the final reward) value. Try it with the example above !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "240d256b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table.maze {\n",
       "    border-collapse: collapse;\n",
       "}\n",
       "\n",
       "td {\n",
       "    text-align: center;\n",
       "}\n",
       "\n",
       "table.maze td.cell {\n",
       "    position: relative;\n",
       "    border: 1px solid black;\n",
       "}\n",
       "\n",
       "table.maze td.cell div.agent {\n",
       "    position: absolute;\n",
       "    left: 25%;\n",
       "    top: 25%;;\n",
       "    width: 50%;\n",
       "    height: 50%;\n",
       "    border-radius: 100%;\n",
       "    background:rgba(69, 100, 186, 0.5);\n",
       "}\n",
       "\n",
       "\n",
       "td.wall {\n",
       "    background: black;\n",
       "}\n",
       "\n",
       "td.terminal {\n",
       "    background: rgb(246, 170, 246);\n",
       "}\n",
       "\n",
       "table.maze table td {\n",
       "    width: .5rem;\n",
       "    height: .5rem;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "table.maze table td.arrow {\n",
       "    color: transparent;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       "table.maze table td.value {\n",
       "    position: relative;\n",
       "    width: 2rem;\n",
       "    height: 2rem;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e2c2f5df7240c799c06cf681ffbbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MazeWidget(cells=array([[ 0,  2,  5,  7],\n",
       "       [-1,  3,  6,  8],\n",
       "       [ 1,  4, -1,  9]]), t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [[STUDENT]]...\n",
    "\n",
    "epsilon = 0.02\n",
    "start_q, start_q_list, time_list = q_learning_eps(\n",
    "    env, epsilon, learning_rate=0.5, init_q=1e-3, nb_episodes=MAX_EPISODES, uniform=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b098a",
   "metadata": {},
   "source": [
    "### Learning dynamics\n",
    "\n",
    "By watching carefully the values while the agent is learning, you can see that\n",
    "the agent favors certains paths over others which have a strictly equivalent value.\n",
    "This can be explained easily: as the agent chooses a path for the first\n",
    "time, it updates the values along that path, these values get higher than the\n",
    "surrounding values, and the agent chooses the same path again and again,\n",
    "increasing the phenomenon. Only steps of random exploration can counterbalance\n",
    "this effect, but they do so extremely slowly.\n",
    "\n",
    "### Exploration\n",
    "\n",
    "In the `q_learning(...)` function above, action selection is based on a\n",
    "$\\epsilon$-greedy policy. Instead, it could have relied on *`softmax`*.\n",
    "\n",
    "In the function below, you have to replace the call to the\n",
    "previous *$\\epsilon$-greedy* policy with a `softmax` policy. The\n",
    "`softmax(...)` and `egreedy(...)` functions are available in\n",
    "`mazemdp.toolbox`.\n",
    "\n",
    "`sofmax(...)` returns a distribution probability over actions. To sample\n",
    "an action according to their probabilities, you can use the\n",
    "`sample_categorical` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98f6c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Q-Learning softmax version ----------------------------#\n",
    "# Given a temperature \"beta\", the QLearning algorithm computes the state action-value function\n",
    "# based on a softmax policy\n",
    "# alpha is the learning rate\n",
    "\n",
    "\n",
    "def q_learning_soft(\n",
    "    mdp: MazeMDPEnv,\n",
    "    beta: float,\n",
    "    nb_episodes: int = 20,\n",
    "    render: bool = True,\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    mdp = env.unwrapped\n",
    "\n",
    "    # Initialize the state-action value function\n",
    "    # alpha is the learning rate\n",
    "    q = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_min = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_list = []\n",
    "    time_list = []\n",
    "\n",
    "    # Run learning cycle\n",
    "    mdp = env.unwrapped\n",
    "    if render:\n",
    "        mdp.init_draw(\"Q Learning (Softmax)\")\n",
    "\n",
    "    for _ in range(nb_episodes):\n",
    "        # Draw the first state of episode i using a uniform distribution over all the states\n",
    "        s, _ = env.reset(options={\"uniform\": True})\n",
    "        cpt = 0\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        while not (terminated or truncated):\n",
    "            if render:\n",
    "                mdp.draw_v_pi(q, q.argmax(axis=1))\n",
    "\n",
    "            # [[STUDENT]]...\n",
    "\n",
    "            # Draw an action using a soft-max policy\n",
    "            a = sample_categorical(softmax(q, s, beta))\n",
    "\n",
    "\n",
    "            # [[STUDENT]]...\n",
    "            # Perform a step of the MDP\n",
    "            s_next, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "            # [[STUDENT]]...\n",
    "\n",
    "            # Update the state-action value function with q-Learning\n",
    "            delta = (r + mdp.gamma * (1-terminated) * np.max(q[s_next, :])) - q[s, a]\n",
    "            q[s,a] = q[s,a] + learning_rate * delta\n",
    "\n",
    "            # Update the agent position\n",
    "            s = s_next\n",
    "            cpt = cpt + 1\n",
    "\n",
    "        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n",
    "        time_list.append(cpt)\n",
    "\n",
    "    if render:\n",
    "        mdp.current_state = 0\n",
    "        mdp.draw_v_pi(q, get_policy_from_q(q))\n",
    "\n",
    "    return q, q_list, time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac41143",
   "metadata": {},
   "source": [
    " Run this new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60c071da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table.maze {\n",
       "    border-collapse: collapse;\n",
       "}\n",
       "\n",
       "td {\n",
       "    text-align: center;\n",
       "}\n",
       "\n",
       "table.maze td.cell {\n",
       "    position: relative;\n",
       "    border: 1px solid black;\n",
       "}\n",
       "\n",
       "table.maze td.cell div.agent {\n",
       "    position: absolute;\n",
       "    left: 25%;\n",
       "    top: 25%;;\n",
       "    width: 50%;\n",
       "    height: 50%;\n",
       "    border-radius: 100%;\n",
       "    background:rgba(69, 100, 186, 0.5);\n",
       "}\n",
       "\n",
       "\n",
       "td.wall {\n",
       "    background: black;\n",
       "}\n",
       "\n",
       "td.terminal {\n",
       "    background: rgb(246, 170, 246);\n",
       "}\n",
       "\n",
       "table.maze table td {\n",
       "    width: .5rem;\n",
       "    height: .5rem;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "table.maze table td.arrow {\n",
       "    color: transparent;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       "table.maze table td.value {\n",
       "    position: relative;\n",
       "    width: 2rem;\n",
       "    height: 2rem;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfeb88b3ee149f4af34ad058b74be47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MazeWidget(cells=array([[ 0,  2,  5,  7],\n",
       "       [-1,  3,  6,  8],\n",
       "       [ 1,  4, -1,  9]]), t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NB_EPISODES = 40\n",
    "beta = 0.16\n",
    "q, q_list, time_list = q_learning_soft(env, beta, nb_episodes=MAX_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124441c",
   "metadata": {},
   "source": [
    "# Sarsa\n",
    "\n",
    "The **SARSA** algorithm is very similar to **Q-learning**. At first glance,\n",
    "the only difference is in the update rule. However, to perform the update in\n",
    "**SARSA**, one needs to know the action the agent will take when it will be at\n",
    "the next state, even if the agent is taking a random action.\n",
    "\n",
    "This implies that the next state action is determined in advance and stored\n",
    "for being played at the next time step.\n",
    "\n",
    "The update formula is as follows:\n",
    "\n",
    "$$ \\delta_t = \\left( r(s_t,a_t) + \\gamma Q^{(i)}(s_{t+1}, a_{t+1})\n",
    "\\right) -Q^{(i)}(s_t,a_t) $$\n",
    "\n",
    "$$ Q^{(i+1)}(s_t,a_t) = Q^{(i)}(s_t,a_t) + \\alpha \\delta_t $$\n",
    "\n",
    "\n",
    "## SARSA ($\\epsilon-greedy$ version)\n",
    "Fill the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fa67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an exploration rate epsilon, the SARSA algorithm computes the state action-value function\n",
    "# based on an epsilon-greedy policy\n",
    "# alpha is the learning rate\n",
    "\n",
    "\n",
    "def sarsa_eps(\n",
    "    mdp: MazeMDPEnv,\n",
    "    epsilon: float,\n",
    "    nb_episodes: int = 20,\n",
    "    render: bool = True,\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    mdp = env.unwrapped\n",
    "\n",
    "    # Initialize the state-action value function\n",
    "    # alpha is the learning rate\n",
    "    q = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_min = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_list = []\n",
    "    time_list = []\n",
    "\n",
    "    # Run learning cycle\n",
    "\n",
    "    if render:\n",
    "        mdp.init_draw(\"SARSA e-greedy\")\n",
    "\n",
    "    a = egreedy(q, s, epsilon)\n",
    "    for _ in range(nb_episodes):\n",
    "        # Draw the first state of episode i using a uniform distribution over all the states\n",
    "        s, _ = env.reset(options={\"uniform\": True})\n",
    "        cpt = 0\n",
    "\n",
    "        # [[STUDENT]]...\n",
    "\n",
    "        s_next, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "        a_next = egreedy(q, s_next, epsilon)\n",
    "\n",
    "        delta = (r + mdp.gamma * (1-terminated) * q[s_next, a_next]) - q[s, a]\n",
    "        q[s,a] = q[s,a] + learning_rate * delta\n",
    "\n",
    "        s = s_next\n",
    "        cpt = cpt + 1\n",
    "\n",
    "        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n",
    "        time_list.append(cpt)\n",
    "\n",
    "    if render:\n",
    "        mdp.current_state = 0\n",
    "        mdp.draw_v_pi(q, get_policy_from_q(q))\n",
    "    return q_list, time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f2fa5",
   "metadata": {},
   "source": [
    "And run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91478bbf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epsilon = 0.02\n",
    "q, q_list, time_list = sarsa_eps(env, epsilon, nb_episodes=MAX_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541034d",
   "metadata": {},
   "source": [
    "As for **Q-learning** above, copy-paste the resulting code to get a\n",
    "*sarsa_soft(...)* and a *sarsa_eps(...)* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547a8e4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --------------------------- SARSA, softmax version -------------------------------#\n",
    "# Given a temperature \"beta\", the SARSA algorithm computes the state action-value function\n",
    "# based on a softmax policy\n",
    "# alpha is the learning rate\n",
    "\n",
    "\n",
    "def sarsa_soft(\n",
    "    mdp: MazeMDPEnv,\n",
    "    beta: float = 0.1,\n",
    "    nb_episodes: int = 20,\n",
    "    render: bool = True,\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    mdp = env.unwrapped\n",
    "\n",
    "    # Initialize the state-action value function\n",
    "    # alpha is the learning rate\n",
    "    q = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_min = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_list = []\n",
    "    time_list = []\n",
    "\n",
    "    # Run learning cycle\n",
    "\n",
    "    if render:\n",
    "        mdp.init_draw(\"SARSA (Softmax)\")\n",
    "\n",
    "    for _ in range(nb_episodes):\n",
    "        # Draw the first state of episode i using a uniform distribution over all the states\n",
    "        x, _ = env.reset(options={\"uniform\": True})\n",
    "        cpt = 0\n",
    "\n",
    "        # [[STUDENT]]...\n",
    "\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n",
    "        time_list.append(cpt)\n",
    "\n",
    "    if render:\n",
    "        mdp.current_state = 0\n",
    "        mdp.draw_v_pi(q, get_policy_from_q(q))\n",
    "    return q_list, time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d4dc5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "And run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01db861",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# [[STUDENT]]...\n",
    "\n",
    "# Put your code to run sarsa_soft here\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cacf8e",
   "metadata": {},
   "source": [
    "## Impact of `epsilon` and `temperature` on Q-learning and SARSA\n",
    "\n",
    "Compare the number of steps needed by **Q-learning** and **SARSA** to converge\n",
    "on a given MDP using the *softmax* and *$\\epsilon$-greedy* exploration\n",
    "strategies. To figure out, you can use the provided `plot_ql_sarsa(m, alpha, epsilon,\n",
    "beta, nb_episodes, alpha, render)` function below with various values\n",
    "for $\\epsilon$ (e.g. 0.001, 0.01, 0.1) and $\\beta$ (e.g. 0.1, 5, 10) and\n",
    "comment the obtained curves. Other visualizations are welcome, e.g. a heat map, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40255b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- plot learning curves of Q-Learning and Sarsa using epsilon-greedy and softmax ----------#\n",
    "def plot_ql_sarsa(env, epsilon, beta, nb_episodes, alpha, render):\n",
    "    q, q_list1, time_list1 = q_learning_eps(env, epsilon, nb_episodes, alpha, render)\n",
    "    q, q_list2, time_list2 = q_learning_soft(env, beta, nb_episodes, alpha, render)\n",
    "    q, q_list3, time_list3 = sarsa_eps(env, epsilon, nb_episodes, alpha, render)\n",
    "    q, q_list4, time_list4 = sarsa_soft(env, beta, nb_episodes, alpha, render)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(range(len(q_list1)), q_list1, label=\"Q-learning e-greedy\")\n",
    "    plt.plot(range(len(q_list2)), q_list2, label=\"Q-learning softmax\")\n",
    "    plt.plot(range(len(q_list3)), q_list3, label=\"SARSA e-greedy\")\n",
    "    plt.plot(range(len(q_list4)), q_list4, label=\"SARSA softmax\")\n",
    "\n",
    "    plt.xlabel(\"Number of episodes\")\n",
    "    plt.ylabel(\"Norm of Q values\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    # plt.savefig(\"comparison_RL.png\")\n",
    "    plt.title(\"Comparison of convergence rates\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(time_list1)), time_list1, label=\"qlearning e-greedy\")\n",
    "    plt.plot(range(len(time_list2)), time_list2, label=\"qlearning softmax\")\n",
    "    plt.plot(range(len(time_list3)), time_list3, label=\"SARSA e-greedy\")\n",
    "    plt.plot(range(len(time_list4)), time_list4, label=\"SARSA softmax\")\n",
    "\n",
    "    plt.xlabel(\"Number of episodes\")\n",
    "    plt.ylabel(\"Steps to reach goal\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    # plt.savefig(\"comparison_RL.png\")\n",
    "    plt.title(\"test\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434908fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "plot_ql_sarsa(\n",
    "    env, epsilon=0.02, beta=0.16, nb_episodes=MAX_EPISODES0, alpha=0.5, render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d6b242",
   "metadata": {},
   "source": [
    "### Effect of hyper-parameters\n",
    "\n",
    "The other two hyper-parameters of **Q-learning** and **SARSA** are $\\alpha$,\n",
    "and $\\gamma$. By varying the values of these hyper-parameters and watching the\n",
    "learning process and behavior of the agent, explain their impact on the\n",
    "algorithm. Using additional plotting functions is also welcome.\n",
    "\n",
    "A good idea to visualize the effect of two parameters is to generate a heat map\n",
    "by letting both parameters take values in a well-chosen interval.\n",
    "Make sure that your figure complies with [The figure checklist](https://master-dac.isir.upmc.fr/The_figure_checklist.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[STUDENT]]...\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
