{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb235492",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, using BBRL, we code the DDPG algorithm.\n",
    "\n",
    "To understand this code, you need to know more about [the BBRL interaction\n",
    "model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md) Then you\n",
    "should run [a didactical\n",
    "example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/03-multi_env_autoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=True.\n",
    "\n",
    "The DDPG algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=0D6a0a1HTtc) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ddpg.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778e6ed",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Prepare the environment\n",
    "\n",
    "\n",
    "import bbrl_utils\n",
    "\n",
    "bbrl_utils.setup()\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.visu.plot_policies import plot_policy\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25584c49",
   "metadata": {},
   "source": [
    "# Learning environment\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The learning environment is controlled by a configuration providing a few\n",
    "important things as described in the example below. This configuration can\n",
    "hold as many extra information as you need, the example below is the minimal\n",
    "one.\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # This defines the a path for logs and saved models\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # The Gymnasium environment\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # Algorithm\n",
    "    \"algorithm\": {\n",
    "        # Seed used for the random number generator\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # Number of parallel training environments\n",
    "        \"n_envs\": 8,\n",
    "\n",
    "        # Number of transitions to collect at each epoch for an environment.\n",
    "        # This number has to be multiplied by n_envs to get the number of new transitions\n",
    "        # collected at each epoch.\n",
    "        \"n_steps\": 100,\n",
    "\n",
    "        # Number of transitions before starting to train\n",
    "        \"learning_starts\": 10_000,\n",
    "                \n",
    "        # Minimum number of steps between two evaluations\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # Number of parallel evaluation environments\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # Number of epochs (loops)\n",
    "        \"max_epochs\": 40000,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Creates the configuration object, i.e. cfg.algorithm.nb_evals is 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "## The RL algorithm\n",
    "\n",
    "In this notebook, the RL algorithm is based on `EpochBasedAlgo`, that defines\n",
    "the algorithm environment when using episodes. To use such environment, we\n",
    "just need to subclass `EpochBasedAlgo` and to define two things, namely the\n",
    "`train_policy` and the `eval_policy`. Both are BBRL agents that, given the\n",
    "environment state, select the action to perform.\n",
    "\n",
    "```py\n",
    "  class MyAlgo(EpochBasedAlgo):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__(cfg)\n",
    "\n",
    "          # Define the train and evaluation policies\n",
    "          # (the agents compute the workspace `action` variable)\n",
    "          self.train_policy = MyPolicyAgent(...)\n",
    "          self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "The `EpochBasedAlgo` defines useful objects:\n",
    "\n",
    "- `algo.cfg` is the configuration\n",
    "- `algo.nb_steps` (integer) is the number of steps since the training began\n",
    "- `algo.logger` is a logger that can be used to collect statistics during training:\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` registers the `critic_loss` value on tensorboard\n",
    "- `algo.evaluate()` evaluates the current `eval_policy` if needed, and keeps the\n",
    "agent if it was the best so far (average cumulated reward);\n",
    "- `algo.visualize_best()` runs the best agent on one episode, and displays the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d39d83",
   "metadata": {},
   "source": [
    "The [DDPG](https://arxiv.org/pdf/1509.02971.pdf) algorithm is an actor critic\n",
    "algorithm. We use a simple neural network builder function. This neural\n",
    "networks plays the role of the actor and the critic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73702e07",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Definition of agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14ae54",
   "metadata": {},
   "source": [
    "The critic is a neural network taking the state $s$ and action $a$ as input,\n",
    "and its output layer has a unique neuron whose value is the value of being in\n",
    "that state and performing that action $Q(s,a)$.\n",
    "\n",
    "As usual, the ```forward(...)``` function is used to write Q-values in the\n",
    "workspace from time indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd4de5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Get the current state $s_t$ and the chosen action $a_t$\n",
    "        obs = self.get((\"env/env_obs\", t))  # shape B x D_{obs}\n",
    "        action = self.get((\"action\", t))  # shape B x D_{action}\n",
    "\n",
    "        # Compute the Q-value(s_t, a_t)\n",
    "        obs_act = torch.cat((obs, action), dim=1)  # shape B x (D_{obs} + D_{action})\n",
    "        # Get the q-value (and remove the last dimension since it is a scalar)\n",
    "        q_value = self.model(obs_act).squeeze(-1)\n",
    "        self.set((f\"{self.prefix}q_value\", t), q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ea5ea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The actor is also a neural network, it takes a state $s$ as input and outputs\n",
    "an action $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20f78f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ContinuousDeterministicActor(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        layers = [state_dim] + list(hidden_layers) + [action_dim]\n",
    "        self.model = build_mlp(\n",
    "            layers, activation=nn.ReLU(), output_activation=nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.model(obs)\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab52179",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Creating an Exploration method\n",
    "\n",
    "In the continuous action domain, basic exploration differs from the methods\n",
    "used in the discrete action domain. Here we generally add some Gaussian noise\n",
    "to the output of the actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dad4d9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(Agent):\n",
    "    def __init__(self, sigma):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        dist = Normal(act, self.sigma)\n",
    "        action = dist.sample()\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd5da8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In [the original DDPG paper](https://arxiv.org/pdf/1509.02971.pdf), the\n",
    "authors rather used the more sophisticated Ornstein-Uhlenbeck noise where\n",
    "noise is correlated between one step and the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddOUNoise(Agent):\n",
    "    \"\"\"\n",
    "    Ornstein-Uhlenbeck process noise for actions as suggested by DDPG paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, std_dev, theta=0.15, dt=1e-2):\n",
    "        self.theta = theta\n",
    "        self.std_dev = std_dev\n",
    "        self.dt = dt\n",
    "        self.x_prev = 0\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (act - self.x_prev) * self.dt\n",
    "            + self.std_dev * math.sqrt(self.dt) * torch.randn(act.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        self.set((\"action\", t), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0ef77",
   "metadata": {},
   "source": [
    "### Compute critic loss\n",
    "\n",
    "Detailed explanations of the function to compute the critic loss\n",
    "are given in [the DQN notebook](http://master-dac.isir.upmc.fr/rld/rl/03-1-dqn-introduction.student.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f330bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the (Torch) mse loss function\n",
    "# `mse(x, y)` computes $\\|x-y\\|^2$\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "def compute_critic_loss(\n",
    "    cfg,\n",
    "    reward: torch.Tensor,\n",
    "    must_bootstrap: torch.Tensor,\n",
    "    q_values: torch.Tensor,\n",
    "    target_q_values: torch.Tensor,\n",
    "):\n",
    "    \"\"\"Compute the DDPG critic loss from a sample of transitions\n",
    "\n",
    "    :param cfg: The configuration\n",
    "    :param reward: The reward (shape 2xB)\n",
    "    :param must_bootstrap: Must bootstrap flag (shape 2xB)\n",
    "    :param q_values: The computed Q-values (shape 2xB)\n",
    "    :param target_q_values: The Q-values computed by the target critic (shape 2xB)\n",
    "    :return: the loss (a scalar)\n",
    "    \"\"\"\n",
    "    # Compute temporal difference\n",
    "    # [[STUDENT]]...\n",
    "\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce79a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Compute actor loss\n",
    "The actor loss is straightforward. We want the actor to maximize Q-values, thus we minimize the mean of negated Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b61e9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_actor_loss(q_values):\n",
    "    \"\"\"Returns the actor loss\n",
    "\n",
    "    :param q_values: The q-values (shape 2xB)\n",
    "    :return: A scalar (the loss)\n",
    "    \"\"\"\n",
    "    # [[STUDENT]]...\n",
    "\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4de11b",
   "metadata": {},
   "source": [
    "### Create the DDPG agent\n",
    "\n",
    "In the next cell, we create the critic and the actor, but also an exploration\n",
    "agent to add noise and a target critic. The version below does not use a\n",
    "target actor as it proved hard to tune, but such a target actor is used in the\n",
    "original paper.\n",
    "\n",
    "Note the use of `with_prefix` for agents, that is used to distinguish the\n",
    "agents of the same class. In the code below, this is used to distinguish the\n",
    "`critic` (that writes into `critic/q_value`) with the `target-critic` (that\n",
    "writes into `target-critic/q_value`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b64916",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class DDPG(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # we create the critic and the actor, but also an exploration agent to\n",
    "        # add noise and a target critic. The version below does not use a target\n",
    "        # actor as it proved hard to tune, but such a target actor is used in\n",
    "        # the original paper.\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "\n",
    "        self.critic = ContinuousQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "        ).with_prefix(\"critic/\")\n",
    "        self.target_critic = copy.deepcopy(self.critic).with_prefix(\"target-critic/\")\n",
    "\n",
    "        self.actor = ContinuousDeterministicActor(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # As an alternative, you can use `AddOUNoise`\n",
    "        noise_agent = AddGaussianNoise(cfg.algorithm.action_noise)\n",
    "\n",
    "        self.train_policy = Agents(self.actor, noise_agent)\n",
    "        self.eval_policy = self.actor\n",
    "\n",
    "        # [[STUDENT]]...\n",
    "\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Configure the optimizer\n",
    "        self.actor_optimizer = setup_optimizer(cfg.actor_optimizer, self.actor)\n",
    "        self.critic_optimizer = setup_optimizer(cfg.critic_optimizer, self.critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9745e0",
   "metadata": {},
   "source": [
    "## Main training loop\n",
    "\n",
    "In the following, we code the main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7a03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ddpg(ddpg: DDPG):\n",
    "    for rb in ddpg.iter_replay_buffers():\n",
    "        # Get a sample of transitions (shapes 2x...) from the replay buffer\n",
    "        rb_workspace = rb.get_shuffled(ddpg.cfg.algorithm.batch_size)\n",
    "\n",
    "        # Compute the critic loss\n",
    "\n",
    "        # Critic update\n",
    "        # Compute critic loss\n",
    "        critic loss = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Gradient step (critic)\n",
    "        ddpg.logger.add_log(\"critic_loss\", critic_loss, ddpg.nb_steps)\n",
    "        ddpg.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            ddpg.critic.parameters(), ddpg.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        ddpg.critic_optimizer.step()\n",
    "\n",
    "        # Compute the actor loss\n",
    "\n",
    "        actor_loss = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Gradient step (actor)\n",
    "        ddpg.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            ddpg.actor.parameters(), ddpg.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        ddpg.actor_optimizer.step()\n",
    "\n",
    "        # Soft update of target q function\n",
    "        soft_update_params(\n",
    "            ddpg.critic, ddpg.target_critic, ddpg.cfg.algorithm.tau_target\n",
    "        )\n",
    "\n",
    "        # Evaluate the actor if needed\n",
    "        if ddpg.evaluate():\n",
    "            if ddpg.cfg.plot_agents:\n",
    "                plot_policy(\n",
    "                    ddpg.actor,\n",
    "                    ddpg.eval_env,\n",
    "                    ddpg.best_reward,\n",
    "                    str(ddpg.base_dir / \"plots\"),\n",
    "                    ddpg.cfg.gym_env.env_name,\n",
    "                    stochastic=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6b25f",
   "metadata": {},
   "source": [
    "# Definition of the parameters\n",
    "\n",
    "The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a\n",
    "tensorboard visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"save_best\": False,\n",
    "    \"base_dir\": \"${gym_env.env_name}/ddpg-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    # Set to true to have an insight on the learned policy\n",
    "    # (but slows down the evaluation a lot!)\n",
    "    \"plot_agents\": True,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 1,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"n_envs\": 1,\n",
    "        \"n_steps\": 100,\n",
    "        \"nb_evals\": 10,\n",
    "        \"discount_factor\": 0.98,\n",
    "        \"buffer_size\": 2e5,\n",
    "        \"batch_size\": 64,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"max_epochs\": 11_000,\n",
    "        # Minimum number of transitions before learning starts\n",
    "        \"learning_starts\": 10000,\n",
    "        \"action_noise\": 0.1,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [400, 300],\n",
    "            \"critic_hidden_size\": [400, 300],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"Pendulum-v1\",\n",
    "    },\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9416cae",
   "metadata": {},
   "source": [
    "## Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11222206",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg = DDPG(OmegaConf.create(params))\n",
    "run_ddpg(ddpg)\n",
    "ddpg.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91e373",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Starting from the above version , you should code [the TD3\n",
    "algorithm](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf).\n",
    "\n",
    "## Differences with DDPG\n",
    "\n",
    "### Target policy smoothing\n",
    "\n",
    "The noise is clipped\n",
    "\n",
    "### Clipped double-Q learning\n",
    "\n",
    "The target value fir the Bellman backup takes the min over two target critics:\n",
    "\n",
    "$$y(r, s', d) = r + \\gamma (1 - d) \\min_{i=1,2} {Q'}_{\\phi_{i, \\text{targ}}}(s', a'(s'))$$\n",
    "\n",
    "where $d$ tells whether the episode is terminated and both Q-value estimators are learned using the following loss:\n",
    "\n",
    "$$L(\\phi_1, {\\mathcal D}) = \\mathbb{E}_{(s,a,r,s',d) \\sim {\\mathcal D}}{\\Bigg( Q_{\\phi_1}(s,a) - y(r,s',d) \\Bigg)^2}$$\n",
    "$$L(\\phi_2, {\\mathcal D}) = \\mathbb{E}_{(s,a,r,s',d) \\sim {\\mathcal D}}{\\Bigg( Q_{\\phi_2}(s,a) - y(r,s',d) \\Bigg)^2}.$$\n",
    "\n",
    "### Policy learning\n",
    "\n",
    "We learn the policy with a loss computed using the first critic ($\\Phi_1$)\n",
    "$$\\max_{\\theta} \\underset{s \\sim {\\mathcal D}}{{\\mathrm E}}\\left[ Q_{\\phi_1}(s, \\mu_{\\theta}(s)) \\right]$$\n",
    "\n",
    "## Algorithm details\n",
    "\n",
    "For that, you need to use two critics (and two target critics) and always take\n",
    "the minimum output between the two when you ask for the Q-value of a (state,\n",
    "action) pair.\n",
    "\n",
    "In more detail, you have to do the following:\n",
    "- replace the single critic and corresponding target critic with two critics\n",
    "  and target critics (name them ```critic_1, critic_2, target_critic_1,\n",
    "  target_critic_2```)\n",
    "- get the q-values and target q-values corresponding to all these critics.\n",
    "- then the target q-values you should consider to update the critic should be\n",
    "  the minimum over the target q-values at each step (use ```torch.min(...)``` to\n",
    "  get this min over a sequence of data).\n",
    "- to update the actor, do it with the q-values of an arbitrarily chosen\n",
    "  critic, e.g. critic_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebe3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # Define the agents and optimizers for TD3\n",
    "\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_td3(td3: TD3):\n",
    "    for rb in td3.iter_replay_buffers():\n",
    "        rb_workspace = rb.get_shuffled(td3.cfg.algorithm.batch_size)\n",
    "\n",
    "        # Implement the learning loop\n",
    "\n",
    "        assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5f092",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"save_best\": False,\n",
    "    \"base_dir\": \"${gym_env.env_name}/td3-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    # Set to true to have an insight on the learned policy\n",
    "    # (but slows down the evaluation a lot!)\n",
    "    \"plot_agents\": True,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 1,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"n_envs\": 1,\n",
    "        \"n_steps\": 100,\n",
    "        \"nb_evals\": 10,\n",
    "        \"discount_factor\": 0.98,\n",
    "        \"buffer_size\": 2e5,\n",
    "        \"batch_size\": 64,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"max_epochs\": 11_000,\n",
    "        # Minimum number of transitions before learning starts\n",
    "        \"learning_starts\": 10000,\n",
    "        \"action_noise\": 0.1,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [400, 300],\n",
    "            \"critic_hidden_size\": [400, 300],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"Pendulum-v1\",\n",
    "    },\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "}\n",
    "\n",
    "td3 = TD3(OmegaConf.create(params))\n",
    "run_td3(td3)\n",
    "td3.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a96e2b",
   "metadata": {},
   "source": [
    "## Experimental comparison\n",
    "\n",
    "Take an environment where the over-estimation bias may matter, and compare the\n",
    "performance of DDPG and TD3. Visualize the Q-value long before convergence to\n",
    "see whether indeed DDPG overestimates the Q-values with respect to TD3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7fa5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbrl.stats import WelchTTest\n",
    "\n",
    "WelchTTest().plot(\n",
    "    torch.stack(ddpg.eval_rewards),\n",
    "    torch.stack(td3.eval_rewards),\n",
    "    legends=\"ddpg/td3\",\n",
    "    save=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
