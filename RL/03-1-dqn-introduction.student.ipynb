{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60158931",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    " Copyright Â© Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737cd89e",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "In this notebook, using BBRL, we code a simple version of the DQN algorithm\n",
    "without a replay buffer nor a target network so as to better understand the\n",
    "inner mechanisms.\n",
    "\n",
    "To understand this code, you need to know more about [the BBRL interaction\n",
    "model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md) Then you\n",
    "should run [a didactical\n",
    "example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/02-multi_env_noautoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=False.\n",
    "\n",
    "The DQN algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=CXwvOMJujZk) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/dqn.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd12ea8c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\titouan\\OneDrive\\Bureau\\M2-MIND\\RL\\venv\\Lib\\site-packages\\bbrl_utils\\notebook.py:46: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # noqa: F401\n"
     ]
    }
   ],
   "source": [
    "# Prepare the environment\n",
    "\n",
    "\n",
    "import bbrl_utils\n",
    "\n",
    "bbrl_utils.setup()\n",
    "\n",
    "import os\n",
    "\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents\n",
    "from bbrl_utils.algorithms import EpisodicAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14119d0c",
   "metadata": {},
   "source": [
    "# Learning environment\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The learning environment is controlled by a configuration that define a few\n",
    "important things as described in the example below. This configuration can\n",
    "hold as many extra information as you need, the example below is the minimal\n",
    "one.\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # This defines the a path for logs and saved models\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # The Gymnasium environment\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # Algorithm\n",
    "    \"algorithm\": {\n",
    "        # Seed used for the random number generator\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # Number of parallel training environments\n",
    "        \"n_envs\": 8,\n",
    "                \n",
    "        # Minimum number of steps between two evaluations\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # Number of parallel evaluation environments\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # Number of epochs (loops)\n",
    "        \"max_epochs\": 40000,\n",
    "\n",
    "    },\n",
    "}\n",
    "\n",
    "# Creates the configuration object, i.e. cfg.algorithm.nb_evals is 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "## The RL algorithm\n",
    "\n",
    "In this notebook, the RL algorithm is based on `EpisodicAlgo`, that defines\n",
    "the algorithm environment when using episodes. To use such environment, we\n",
    "just need to subclass `EpisodicAlgo` and to define two things, namely the\n",
    "`train_policy` and the `eval_policy`. Both are BBRL agents that, given the\n",
    "environment state, select the action to perform.\n",
    "\n",
    "```py\n",
    "  class MyAlgo(EpisodicAlgo):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__(cfg)\n",
    "\n",
    "          # Define the train and evaluation policies\n",
    "          # (the agents compute the workspace `action` variable)\n",
    "          self.train_policy = MyPolicyAgent(...)\n",
    "          self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "The `EpisodicAlgo` defines useful objects:\n",
    "\n",
    "- `algo.cfg` is the configuration\n",
    "- `algo.nb_steps` (integer) is the number of steps since the training began\n",
    "- `algo.logger` is a logger that can be used to collect statistics during training:\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` registers the `critic_loss` value on tensorboard\n",
    "- `algo.evaluate()` evaluates the current `eval_policy` if needed, and keeps the\n",
    "agent if it was the best so far (average cumulated reward);\n",
    "- `algo.visualize_best()` runs the best agent on one episode, and displays the video\n",
    "\n",
    "\n",
    "\n",
    "Besides, it also defines an `iter_episodes` is simple:\n",
    "\n",
    "```py\n",
    "  # With episodes\n",
    "  for workspace in rl_algo.iter_episodes():\n",
    "      # workspace is a workspace containing transitions\n",
    "      # Episodes shorter than the longer one contain duplicated\n",
    "      # transitions (with `env/done` set to true)\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea5037",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Definition of agents\n",
    "\n",
    "The [DQN](https://daiwk.github.io/assets/dqn.pdf) algorithm is a critic only\n",
    "algorithm. Thus we just need a Critic agent (which is also used to output\n",
    "actions) and an Environment agent.\n",
    "\n",
    "### The critic agent\n",
    "\n",
    "The critic agent is an instance of the `DiscreteQAgent` class. We first build\n",
    "a deterministic neural network that takes the state as input (so it has one\n",
    "input neuron per state variable) and that outputs the Q-value of each action\n",
    "in that state (so it has one output neuron per action)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4179575",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "As any BBRL agent, the DiscreteQAgent has a `forward()` function that takes a\n",
    "time state as input. This `forward()` function outputs the Q-values of all\n",
    "actions at the corresponding time step. Additionally, if the critic is used to\n",
    "choose an action, it also outputs the chosen action at the same time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21c92b3e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class DiscreteQAgent(Agent):\n",
    "    \"\"\"BBRL agent (discrete actions) based on a MLP\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = build_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        \"\"\"An Agent can use self.workspace\"\"\"\n",
    "\n",
    "        # Retrieves the observation from the environment at time t\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "\n",
    "        # Computes the critic (Q) values for the observation\n",
    "        q_values = self.model(obs)\n",
    "\n",
    "        # ... and sets the q-values (one for each possible action)\n",
    "        self.set((\"q_values\", t), q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1faa8",
   "metadata": {},
   "source": [
    "#### Greedily choosing the action\n",
    "\n",
    "The ArgmaxActionSelector is in charge of choosing the action whose Q-value is\n",
    "the highest given the Q-values of all actions. We may use it when we do not\n",
    "want to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0067da03",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ArgmaxActionSelector(Agent):\n",
    "    \"\"\"BBRL agent that selects the best action based on Q(s,a)\"\"\"\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        q_values = self.get((\"q_values\", t))\n",
    "        action = q_values.argmax(1)\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0dfe70",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Creating an Exploration method\n",
    "\n",
    "As Q-learning, DQN needs some exploration to prevent too early convergence.\n",
    "Here we use the simple $\\epsilon$-greedy exploration method.\n",
    "It is implemented as an agent which chooses an action based on the Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ada2a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class EGreedyActionSelector(Agent):\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        # Retrieves the q values\n",
    "        # (matrix nb. of episodes x nb. of actions)\n",
    "        q_values: torch.Tensor = self.get((\"q_values\", t))\n",
    "        size, nb_actions = q_values.shape\n",
    "\n",
    "        # Flag\n",
    "        is_random = torch.rand(size) < self.epsilon\n",
    "\n",
    "        # Actions (random / argmax)\n",
    "        random_action = torch.randint(nb_actions, size=(size,))\n",
    "        max_action = q_values.argmax(-1)\n",
    "\n",
    "        # Choose the action based on the is_random flag\n",
    "        action = torch.where(is_random, random_action, max_action)\n",
    "\n",
    "        # Sets the action at time t\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16651c8",
   "metadata": {},
   "source": [
    "## Heart of the algorithm\n",
    "\n",
    "### Computing the critic loss\n",
    "\n",
    "The role of the `compute_critic_loss` function is to implement the Bellman\n",
    "backup rule. In Q-learning, this rule was written:\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha [ r(s_t,a_t) + \\gamma \\max_a\n",
    "Q(s_{t+1},a) - Q(s_t,a_t)]$$\n",
    "\n",
    "In DQN, the update rule $Q \\leftarrow Q + \\alpha [\\delta] $ is replaced by a\n",
    "gradient descent step over the Q-network.\n",
    "\n",
    "We first compute a target value: $ target = r(s_t,a_t) + \\gamma \\max_a\n",
    "Q(s_{t+1},a)$ from a set of samples.\n",
    "\n",
    "Then we get a TD error $\\delta$ by substracting $Q(s_t,a_t)$ for these\n",
    "samples, and we use the squared TD error as a loss function: $ loss = (target\n",
    "- Q(s_t,a_t))^2$.\n",
    "\n",
    "To implement the above calculation in BBRL, the difficulty is to properly deal\n",
    "with time indexes.\n",
    "\n",
    "The `compute_critic_loss` function receives rewards, q_values and actions as\n",
    "tensors that have been computed over a complete episode.\n",
    "\n",
    "We need to take `reward[1:]`, which means all the rewards except the first one\n",
    "because the reward from $(s_t, a_t)$ is $r_{t+1}$. Similarly, to get $\\max_a\n",
    "Q(s_{t+1}, a)$, we need to ignore the first of the max_q values, using\n",
    "`max_q[1:]`.\n",
    "\n",
    "Do not forget to apply .detach() when computing the values of $\\max_a\n",
    "Q(s_{t+1}, a)$, as **we do not want to apply gradient descent on this $\\max_a\n",
    "Q(s_{t+1}, a)$**, we only apply gradient descent to $Q(s_t, a_t)$ according to\n",
    "this target value. In practice, `x.detach()` detaches a computation graph from\n",
    "a tensor, so it avoids computing a gradient over this tensor.\n",
    "\n",
    "The `must_bootstrap` tensor is used as a trick to deal with terminal states,\n",
    "as explained\n",
    "[here](https://github.com/osigaud/bbrl/blob/master/docs/time_limits.md) In\n",
    "practice, `must_bootstrap` is the logical negation of `terminated`. In the\n",
    "autoreset=False version we use full episodes, thus `must_bootstrap` is always\n",
    "True for all steps but the last one.\n",
    "\n",
    "To compute $Q(s_t,a_t)$ we use the [`torch.gather()`](https://pytorch.org/docs/stable/generated/torch.gather.html) function. This function is\n",
    "a little tricky to use, see [this\n",
    "page](https://github.com/osigaud/bbrl/blob/master/docs/using_gather.md) for\n",
    "useful explanations.\n",
    "\n",
    "In particular, the q_vals output that we get is not properly conditioned,\n",
    "hence the need for the `qval[:-1]` (we ignore the last dimension). Finally we\n",
    "just need to compute the difference target - qvals, square it, take the mean\n",
    "and send it back as the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e4364c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_critic_loss(\n",
    "    cfg,\n",
    "    reward: torch.Tensor,\n",
    "    must_bootstrap: torch.Tensor,\n",
    "    done: torch.Tensor,\n",
    "    q_values: torch.Tensor,\n",
    "    action: torch.LongTensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the temporal difference loss from a dataset to\n",
    "    update a critic\n",
    "\n",
    "    For the tensor dimensions:\n",
    "\n",
    "    - T = maximum number of time steps\n",
    "    - B = number of episodes run in parallel\n",
    "    - A = action space dimension\n",
    "\n",
    "    :param cfg: The configuration\n",
    "    :param reward: A (T x B) tensor containing the rewards\n",
    "    :param must_bootstrap: a (T x B) tensor containing 0 at (t, b) if the\n",
    "        episode b was terminated at time $t$ (or before)\n",
    "    :param done: a (T x B) tensor containing 0 at (t, b) if the\n",
    "        episode b is truncated or terminated at time $t$ (or before)\n",
    "    :param q_values: a (T x B x A) tensor containing the Q-values at each time\n",
    "        step, and for each action\n",
    "    :param action: a (T x B) long tensor containing the chosen action\n",
    "\n",
    "    :return: The DQN loss\n",
    "    \"\"\"\n",
    "    # We compute the max of Q-values over all actions and detach (so that this\n",
    "    # part of the computation graph is not included in the gradient\n",
    "    # backpropagation)\n",
    "\n",
    "    max_q = q_values.max(-1).values.detach()\n",
    "    target = reward[1:] + cfg[\"algorithm\"][\"discount_factor\"] * (max_q[1:] * must_bootstrap[1:].int())\n",
    "\n",
    "    qsa_t = q_values.gather(dim=2, index=action.unsqueeze(-1)).squeeze(-1)\n",
    "    not_done = (1 - done[:-1].int())\n",
    "    td = (target - qsa_t[:-1]) ** 2 * not_done\n",
    "    critic_loss = td.sum() / not_done.sum()\n",
    "    return critic_loss\n",
    "\n",
    "    # Compute the loss\n",
    "\n",
    "    # target = r(s_t,a_t) + \\gamma \\max_a Q(s_{t+1},a)\n",
    "    # so .gather() is used to get the q value according to their action (over time and actions)\n",
    "    # the action tensor looks like [a1 a3 a6 a2 etc...], so we want it to match the dimension\n",
    "    # of the q_values tensor, so we .unsqueeze(-1) it to add one parameter -> [[a1][a3]...]\n",
    "    # In our case autoreset=False, so our q_values tensor dim is T x E x A, and we want\n",
    "    # to gather our values over the action, hence the second dim (dimensions start at 0 so we pick 2 not 3)\n",
    "    # now our current .gather() output is something that looks like this: [[q1][q3][q6][q2]etc...]\n",
    "    # it has one extra dimension, so we squeeze(-1) on dimension (same logic as above)\n",
    "    # output =  [q1 q3 q0 q2 etc ...]\n",
    "    qvals = q_values.gather(dim=2, index=action.unsqueeze(-1)).squeeze(-1)\n",
    "    qvals = qvals[:-1]\n",
    "    # print(f'qvals shape: {qvals.shape}')\n",
    "    \n",
    "    # now we look for the max over the q values (but we ignore the first one)\n",
    "    # maxq looks like this:\n",
    "    # torch.tensor([[tensor1][tensor2][tensor3]...])\n",
    "    # so we need to remove that extra dimension with [0]\n",
    "    maxq = q_values[1:].max(dim=2).values\n",
    "    maxq.detach() # we also get the maxq without the first one since we dont need it\n",
    "    # print(f'maxq shape: {maxq.shape}')\n",
    "\n",
    "    \n",
    "    reward = reward[1:] # ignore the first reward\n",
    "    must_bootstrap = must_bootstrap[1:] # drop the last dim\n",
    "\n",
    "    # print(f'reward shape: {reward.shape}')\n",
    "    # print(f'boostrap shape: {must_bootstrap.shape}')\n",
    "\n",
    "    target = reward + cfg[\"algorithm\"][\"discount_factor\"] * (maxq * must_bootstrap)\n",
    "    # print(f'target shape: {target.shape}')\n",
    "\n",
    "    # uncomment this to verify the shapes are the same\n",
    "    # print(target)\n",
    "    # print(qvals)\n",
    "\n",
    "    critic_loss = torch.pow(target - qvals, 2).mean()\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f7117",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Main training loop\n",
    "\n",
    "Note that everything about the shared workspace between all the agents is\n",
    "completely hidden under the hood. This results in a gain of productivity, at\n",
    "the expense of having to dig into the BBRL code if you want to understand the\n",
    "details, change the multiprocessing model, etc.\n",
    "\n",
    "The next cells defines a `EpisodicDQN` that deals with various part of the\n",
    "training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f800e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicDQN(EpisodicAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # Get the observation / action state space dimensions\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "\n",
    "        # Our discrete Q-Agent\n",
    "        self.q_agent = DiscreteQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # The e-greedy strategy (when training)\n",
    "        explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n",
    "\n",
    "        # The training agent combines the Q agent\n",
    "        self.train_policy = Agents(self.q_agent, explorer)\n",
    "\n",
    "        # The optimizer for the Q-Agent parameters\n",
    "        self.optimizer = setup_optimizer(self.cfg.optimizer, self.q_agent)\n",
    "\n",
    "        # ...and the evaluation policy (select the most likely action)\n",
    "        self.eval_policy = Agents(self.q_agent, ArgmaxActionSelector())\n",
    "\n",
    "    def run(self):\n",
    "        for train_workspace in self.iter_episodes():\n",
    "            q_values, terminated, done, reward, action = train_workspace[\n",
    "                \"q_values\", \"env/terminated\", \"env/done\", \"env/reward\", \"action\"\n",
    "            ]\n",
    "\n",
    "            # Determines whether values of the critic should be propagated\n",
    "            # True if the episode reached a time limit or if the task was not done\n",
    "            # See https://github.com/osigaud/bbrl/blob/master/docs/time_limits.md\n",
    "            must_bootstrap = ~terminated\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = compute_critic_loss(\n",
    "                self.cfg, reward, must_bootstrap, done, q_values, action\n",
    "            )\n",
    "\n",
    "            # Store the loss for tensorboard display\n",
    "            self.logger.add_log(\"critic_loss\", critic_loss, self.nb_steps)\n",
    "            dqn.logger.add_log(\n",
    "                \"q_values/min\", q_values.max(-1).values.min(), dqn.nb_steps\n",
    "            )\n",
    "            dqn.logger.add_log(\n",
    "                \"q_values/max\", q_values.max(-1).values.max(), dqn.nb_steps\n",
    "            )\n",
    "            dqn.logger.add_log(\n",
    "                \"q_values/mean\", q_values.max(-1).values.mean(), dqn.nb_steps\n",
    "            )\n",
    "\n",
    "            # Gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.q_agent.parameters(), self.cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Evaluate the current policy (if needed)\n",
    "            self.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fa47e76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch tensorboard from the shell: \n",
      "c:\\Users\\titouan\\OneDrive\\Bureau\\M2-MIND\\RL\\venv\\Scripts/tensorboard --logdir 'c:\\Users\\titouan\\OneDrive\\Bureau\\M2-MIND\\RL\\outputs'\n"
     ]
    }
   ],
   "source": [
    "# We setup tensorboard before running DQN\n",
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c93455a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"save_best\": False,\n",
    "    \"base_dir\": \"${gym_env.env_name}/dqn-simple-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 3,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"epsilon\": 0.1,\n",
    "        \"n_envs\": 8,\n",
    "        \"eval_interval\": 5_000,\n",
    "        \"max_epochs\": 500,\n",
    "        \"nb_evals\": 10,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"architecture\": {\"hidden_size\": [256, 256]},\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPole-v1\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 2e-3,\n",
    "    },\n",
    "}\n",
    "\n",
    "dqn = EpisodicDQN(OmegaConf.create(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac66fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677598d873104504a15acc8442b27e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video of best agent recorded in folder outputs\\CartPole-v1\\dqn-simple-S3_20251006-123229\\best_agent\n",
      "moviepy is not installed, skipping video display\n"
     ]
    }
   ],
   "source": [
    "# Run and visualize the best agent\n",
    "dqn.run()\n",
    "dqn.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544aaa80",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "To get a full DQN, we need to do the following:\n",
    "- Add a replay buffer. We can add a replay buffer independently from the\n",
    "  target network. The version with a replay buffer and no target network\n",
    "  corresponds to [the NQF\n",
    "  algorithm](https://link.springer.com/content/pdf/10.1007/11564096_32.pdf).\n",
    "  This will be the aim of the next notebook.\n",
    "- Before adding the replay buffer, we will first move to a version of DQN\n",
    "  which uses the AutoResetGymAgent. This will be the aim of the next notebook\n",
    "  too.\n",
    "- We should also add a few extra-mechanisms which are present in the full DQN\n",
    "  version: starting to learn once the replay buffer is full enough, decreasing\n",
    "  the exploration rate epsilon...\n",
    "<!-- - We could also add visualization tools to visualize the learned Q network, by using the `plot_critic` function available in [`bbrl.visu.visu_critics`](https://github.com/osigaud/bbrl/blob/master/src/bbrl/visu/visu_critics.py#L13) -->"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
